{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models.signature import infer_signature\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 18:25:17 WARN Utils: Your hostname, sriganesh-Inspiron-14-Plus-7440 resolves to a loopback address: 127.0.1.1; using 172.31.82.137 instead (on interface wlp0s20f3)\n",
      "25/04/20 18:25:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/sriganesh/conda_root/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sriganesh/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sriganesh/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e4c3f2a3-5f45-4c7f-88b3-0a88f285efa4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.4 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 398ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.4 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e4c3f2a3-5f45-4c7f-88b3-0a88f285efa4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/10ms)\n",
      "25/04/20 18:25:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 18:25:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/20 18:25:22 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 18:25:29 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|If you decide to ...|    3|\n",
      "|I've taken a lot ...|    5|\n",
      "|Family diner. Had...|    3|\n",
      "|Wow!  Yummy, diff...|    5|\n",
      "|Cute interior and...|    4|\n",
      "|I am a long term ...|    1|\n",
      "|Loved this tour! ...|    5|\n",
      "|Amazingly amazing...|    5|\n",
      "|This easter inste...|    3|\n",
      "|Had a party of 6 ...|    3|\n",
      "|My experience wit...|    5|\n",
      "|Locals recommende...|    4|\n",
      "|Love going here f...|    4|\n",
      "|Good food--loved ...|    4|\n",
      "|The bun makes the...|    4|\n",
      "|Great place for b...|    5|\n",
      "|Tremendous servic...|    5|\n",
      "|The hubby and I h...|    4|\n",
      "|I go to blow bar ...|    5|\n",
      "|My absolute favor...|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session with Kafka package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YelpKafkaSentiment\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/spark-checkpoint\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType() \\\n",
    "    .add(\"text\", StringType()) \\\n",
    "    .add(\"stars\", DoubleType())\n",
    "\n",
    "# Ingest from Kafka\n",
    "df_raw = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"yelp_reviews\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "df = df_raw.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"label\", col(\"stars\").cast(\"int\")) \\\n",
    "    .drop(\"stars\")\n",
    "\n",
    "# Show the results\n",
    "query = df.writeStream.format(\"console\").outputMode(\"append\").start()\n",
    "time.sleep(30)  # run for 30 seconds\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 18:28:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/20 18:28:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⌛ Buffering for 60s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 18:29:35 WARN TaskSetManager: Stage 2 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 200000 records...\n"
     ]
    }
   ],
   "source": [
    "df.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"training_buffer\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"⌛ Buffering for 60s...\")\n",
    "time.sleep(60)\n",
    "training_df = spark.sql(\"SELECT * FROM training_buffer\")\n",
    "print(f\"Training on {training_df.count()} records...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50)\n",
    "# Create a pipelinea\n",
    "base_stages = [tokenizer, remover, hashingTF, idf]\n",
    "lr_pipeline = Pipeline(stages=base_stages + [lr])\n",
    "rf_pipeline = Pipeline(stages=base_stages + [rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input DataFrame for logging models (replace with your actual input DataFrame)\n",
    "input_example = training_df.limit(5).toPandas()  # Spark DataFrame -> pandas DataFrame\n",
    "\n",
    "# Function to log models with input example and signature\n",
    "def log_model_with_signature(model, model_name):\n",
    "    # Get predictions (or output) for input example\n",
    "    predictions = model.transform(training_df)\n",
    "    \n",
    "    # Infer the model signature from input and output\n",
    "    signature = infer_signature(input_example, predictions.select(\"prediction\").limit(5).toPandas())\n",
    "    \n",
    "    # Log the model with input example and signature\n",
    "    mlflow.spark.log_model(\n",
    "        model,\n",
    "        model_name,\n",
    "        input_example=input_example,\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 18:55:57 WARN TaskSetManager: Stage 226 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:01 WARN TaskSetManager: Stage 228 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:03 WARN TaskSetManager: Stage 230 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:05 WARN TaskSetManager: Stage 232 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:05 WARN TaskSetManager: Stage 234 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:06 WARN TaskSetManager: Stage 236 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:06 WARN TaskSetManager: Stage 238 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:06 WARN TaskSetManager: Stage 240 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:07 WARN TaskSetManager: Stage 242 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:07 WARN TaskSetManager: Stage 244 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:07 WARN TaskSetManager: Stage 246 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:07 WARN TaskSetManager: Stage 248 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:08 WARN TaskSetManager: Stage 250 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:08 WARN TaskSetManager: Stage 252 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:10 WARN TaskSetManager: Stage 254 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:13 WARN TaskSetManager: Stage 256 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:14 WARN TaskSetManager: Stage 259 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:14 WARN TaskSetManager: Stage 260 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:16 WARN TaskSetManager: Stage 261 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:19 WARN TaskSetManager: Stage 263 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_21 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_21 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_7 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_7 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_20 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_20 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_9 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_13 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_13 to disk instead.\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_9 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_0 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_0 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_14 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_14 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_10 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_10 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_12 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_12 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_15 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_15 to disk instead.\n",
      "25/04/20 18:56:23 WARN MemoryStore: Not enough space to cache rdd_773_16 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:23 WARN BlockManager: Persisting block rdd_773_16 to disk instead.\n",
      "25/04/20 18:56:26 WARN MemoryStore: Not enough space to cache rdd_773_0 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:26 WARN MemoryStore: Not enough space to cache rdd_773_20 in memory! (computed 55.2 MiB so far)\n",
      "25/04/20 18:56:26 WARN MemoryStore: Not enough space to cache rdd_773_9 in memory! (computed 55.2 MiB so far)\n",
      "25/04/20 18:56:26 WARN MemoryStore: Not enough space to cache rdd_773_7 in memory! (computed 84.6 MiB so far)\n",
      "25/04/20 18:56:26 WARN MemoryStore: Not enough space to cache rdd_773_14 in memory! (computed 84.6 MiB so far)\n",
      "25/04/20 18:56:27 WARN MemoryStore: Not enough space to cache rdd_773_15 in memory! (computed 130.0 MiB so far)\n",
      "25/04/20 18:56:27 WARN MemoryStore: Not enough space to cache rdd_773_16 in memory! (computed 130.0 MiB so far)\n",
      "25/04/20 18:56:27 WARN MemoryStore: Not enough space to cache rdd_773_10 in memory! (computed 195.3 MiB so far)\n",
      "25/04/20 18:56:27 WARN MemoryStore: Not enough space to cache rdd_773_21 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:28 WARN TaskSetManager: Stage 265 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_15 in memory! (computed 36.5 MiB so far)\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_0 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_16 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_21 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_9 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_10 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_20 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_7 in memory! (computed 36.5 MiB so far)\n",
      "25/04/20 18:56:29 WARN MemoryStore: Not enough space to cache rdd_773_14 in memory! (computed 36.5 MiB so far)\n",
      "25/04/20 18:56:30 WARN TaskSetManager: Stage 267 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_7 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_16 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_10 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_9 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_0 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_14 in memory! (computed 36.5 MiB so far)\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_21 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_20 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:31 WARN MemoryStore: Not enough space to cache rdd_773_15 in memory! (computed 55.2 MiB so far)\n",
      "25/04/20 18:56:33 WARN TaskSetManager: Stage 269 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_9 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_20 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_21 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_16 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_15 in memory! (computed 36.5 MiB so far)\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_0 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_7 in memory! (computed 36.5 MiB so far)\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_14 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:33 WARN MemoryStore: Not enough space to cache rdd_773_10 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:36 WARN DAGScheduler: Broadcasting large task binary with size 1076.0 KiB\n",
      "25/04/20 18:56:36 WARN TaskSetManager: Stage 271 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/20 18:56:37 WARN MemoryStore: Not enough space to cache rdd_773_21 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:37 WARN MemoryStore: Not enough space to cache rdd_773_20 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:37 WARN MemoryStore: Not enough space to cache rdd_773_10 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:37 WARN MemoryStore: Not enough space to cache rdd_773_14 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:37 WARN MemoryStore: Not enough space to cache rdd_773_15 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:37 WARN MemoryStore: Not enough space to cache rdd_773_0 in memory! (computed 24.2 MiB so far)\n",
      "25/04/20 18:56:37 WARN MemoryStore: Not enough space to cache rdd_773_16 in memory! (computed 16.1 MiB so far)\n",
      "25/04/20 18:56:37 WARN MemoryStore: Not enough space to cache rdd_773_9 in memory! (computed 36.5 MiB so far)\n",
      "25/04/20 18:56:38 WARN MemoryStore: Not enough space to cache rdd_773_7 in memory! (computed 55.2 MiB so far)\n",
      "25/04/20 18:56:44 WARN TaskSetManager: Stage 273 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/home/sriganesh/conda_root/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "25/04/20 18:57:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "2025/04/20 18:57:01 INFO mlflow.spark: File '/tmp/tmpudjb9opy/model/sparkml' is already on DFS, copy is not necessary.\n",
      "25/04/20 18:57:05 WARN TaskSetManager: Stage 303 contains a task of very large size (5285 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/home/sriganesh/conda_root/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/04/20 18:57:24 INFO mlflow.spark: File '/tmp/tmph4hyi2b2/model/sparkml' is already on DFS, copy is not necessary.\n",
      "25/04/20 18:57:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/20 18:57:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/04/20 18:57:50 WARN DAGScheduler: Broadcasting large task binary with size 1235.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+-------+-------+----------+\n",
      "|                text|lr_pred|rf_pred|prediction|\n",
      "+--------------------+-------+-------+----------+\n",
      "|So much to like a...|    4.0|    5.0|       4.5|\n",
      "|Our 1st visit her...|    5.0|    5.0|       5.0|\n",
      "|tldr; Huge plates...|    4.0|    5.0|       4.5|\n",
      "|Three chances and...|    1.0|    5.0|       3.0|\n",
      "|Yummy food,  they...|    5.0|    5.0|       5.0|\n",
      "|Very cool bar att...|    4.0|    5.0|       4.5|\n",
      "|Was taken here to...|    5.0|    5.0|       5.0|\n",
      "|I have groen up e...|    5.0|    5.0|       5.0|\n",
      "|I'm not really in...|    5.0|    5.0|       5.0|\n",
      "|Visited Shaving G...|    5.0|    5.0|       5.0|\n",
      "|Very good local i...|    4.0|    5.0|       4.5|\n",
      "|Really like this ...|    4.0|    5.0|       4.5|\n",
      "|I've only been he...|    5.0|    5.0|       5.0|\n",
      "|Where do I start....|    5.0|    5.0|       5.0|\n",
      "|Very cool nostalg...|    5.0|    5.0|       5.0|\n",
      "|Yummy Sushi for a...|    5.0|    5.0|       5.0|\n",
      "|Just had my first...|    5.0|    5.0|       5.0|\n",
      "|I was visiting Tu...|    5.0|    5.0|       5.0|\n",
      "|So disappointed w...|    2.0|    5.0|       3.5|\n",
      "|This place is oka...|    3.0|    5.0|       4.0|\n",
      "+--------------------+-------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"KafkaYelpSentiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"EnsembleModels\"):\n",
    "    lr_model = lr_pipeline.fit(training_df)\n",
    "    rf_model = rf_pipeline.fit(training_df)\n",
    "    log_model_with_signature(lr_model, \"lr_model\")\n",
    "    log_model_with_signature(rf_model, \"rf_model\")\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"Ensemble of LR + RF\")# Re-parse stream\n",
    "predict_df = df_raw.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.text\")\n",
    "\n",
    "# Make predictions from each model\n",
    "lr_pred = lr_model.transform(predict_df).select(\"text\", col(\"prediction\").alias(\"lr_pred\"))\n",
    "rf_pred = rf_model.transform(predict_df).select(\"text\", col(\"prediction\").alias(\"rf_pred\"))\n",
    "\n",
    "# Join on text\n",
    "joined = lr_pred.join(rf_pred, \"text\")\n",
    "\n",
    "# Majority vote\n",
    "final_pred = joined.withColumn(\n",
    "    \"prediction\",\n",
    "    expr(\"int(array(lr_pred, rf_pred)[0] + array(lr_pred, rf_pred)[1]) / 2\")\n",
    ")\n",
    "\n",
    "# Output to console\n",
    "query = final_pred.select(\"text\", \"lr_pred\", \"rf_pred\", \"prediction\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "time.sleep(30)  # run for 30 seconds\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+----------+\n",
      "|               text|lr_pred|rf_pred|prediction|\n",
      "+-------------------+-------+-------+----------+\n",
      "|The food was ok ok!|    3.0|    5.0|         4|\n",
      "+-------------------+-------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 19:02:08 WARN DAGScheduler: Broadcasting large task binary with size 1241.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "custom_text = \"The food was ok ok!\"\n",
    "\n",
    "# Create a DataFrame for custom input\n",
    "custom_df = spark.createDataFrame([(custom_text,)], [\"text\"])\n",
    "\n",
    "# Transform using models\n",
    "lr_pred = lr_model.transform(custom_df).select(\"text\", col(\"prediction\").alias(\"lr_pred\"))\n",
    "rf_pred = rf_model.transform(custom_df).select(\"text\", col(\"prediction\").alias(\"rf_pred\"))\n",
    "\n",
    "# Join predictions on 'text'\n",
    "joined = lr_pred.join(rf_pred, \"text\")\n",
    "\n",
    "# Ensemble prediction (majority vote)\n",
    "final_pred = joined.withColumn(\n",
    "    \"prediction\",\n",
    "    expr(\"int((lr_pred + rf_pred) / 2)\")\n",
    ")\n",
    "\n",
    "# Show the final result\n",
    "final_pred.select(\"text\", \"lr_pred\", \"rf_pred\", \"prediction\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
